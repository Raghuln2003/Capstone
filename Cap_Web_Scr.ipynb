{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7c78136-b78f-4628-a28e-5d0ac82cd30e",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">WEB SCRAPPING</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f280f1-b0b2-48cc-a90b-02a15167bce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "#from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf2b3a8-bfdb-4f65-99b5-fe84072bcdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Chrome options\n",
    "options = Options()\n",
    "#options.add_argument(\"--headless\")  # Run Chrome in headless mode (without opening a window)\n",
    "options.add_argument(\"--disable-gpu\")  # Disable GPU acceleration (optional)\n",
    "\n",
    "# Initialize WebDriver\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d05f63c-a06b-4e21-96c0-a7a107b385d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the Amazon mobile phones store page\n",
    "url = \"https://www.flipkart.com/search?q=refrigerator&sid=j9e%2Cabm%2Chzg&as=on&as-show=on&otracker=AS_QueryStore_OrganicAutoSuggest_1_8_na_na_ps&otracker1=AS_QueryStore_OrganicAutoSuggest_1_8_na_na_ps&as-pos=1&as-type=RECENT&suggestionId=refrigerator%7CRefrigerators&requestId=fadfed73-7da8-4723-94cb-2e33aca85a32&as-searchtext=refridge&sort=price_desc&p%5B%5D=facets.brand%255B%255D%3DCRUISE&p%5B%5D=facets.brand%255B%255D%3DMITASHI&p%5B%5D=facets.brand%255B%255D%3DMOTOROLA&p%5B%5D=facets.brand%255B%255D%3DONIDA&p%5B%5D=facets.brand%255B%255D%3DWalton&p%5B%5D=facets.brand%255B%255D%3DBlue%2BStar&p%5B%5D=facets.brand%255B%255D%3DLifelong&p%5B%5D=facets.brand%255B%255D%3DRockwell&p%5B%5D=facets.brand%255B%255D%3DAdmiral&p%5B%5D=facets.brand%255B%255D%3DSharp&p%5B%5D=facets.brand%255B%255D%3DHitachi&p%5B%5D=facets.brand%255B%255D%3DBLACK%252BDECKER&p%5B%5D=facets.brand%255B%255D%3DKenstar&p%5B%5D=facets.brand%255B%255D%3DAcer&p%5B%5D=facets.brand%255B%255D%3DTOSHIBA&p%5B%5D=facets.brand%255B%255D%3DBPL&p%5B%5D=facets.brand%255B%255D%3DElectrolux&p%5B%5D=facets.brand%255B%255D%3Drealme%2BTechLife&p%5B%5D=facets.brand%255B%255D%3DKelvinator&p%5B%5D=facets.brand%255B%255D%3DLiebherr&p%5B%5D=facets.brand%255B%255D%3DBOSCH&p%5B%5D=facets.brand%255B%255D%3DPanasonic&p%5B%5D=facets.brand%255B%255D%3DLloyd&p%5B%5D=facets.brand%255B%255D%3DMarQ%2Bby%2BFlipkart&p%5B%5D=facets.brand%255B%255D%3DMidea&p%5B%5D=facets.brand%255B%255D%3DCANDY&p%5B%5D=facets.brand%255B%255D%3DVoltas%2BBeko&p%5B%5D=facets.brand%255B%255D%3DIFB&p%5B%5D=facets.brand%255B%255D%3DGodrej&p%5B%5D=facets.brand%255B%255D%3DHaier&p%5B%5D=facets.brand%255B%255D%3DWhirlpool&p%5B%5D=facets.brand%255B%255D%3DLG&p%5B%5D=facets.brand%255B%255D%3DSAMSUNG\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9214dab-3d7e-48d8-bf4b-8c93f77de076",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = driver.page_source\n",
    "soup = BeautifulSoup(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ce7094-7ed2-47d5-a6de-7c66f94a6f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "AC =soup.find_all('div',class_='cPHDOP col-12-12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1372835d-657b-4c20-a40a-4ed6f9cf20cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_product_names = []\n",
    "\n",
    "# Number of pages to scrape (you can adjust this or determine dynamically)\n",
    "num_pages = 39\n",
    "\n",
    "# Loop through each page\n",
    "for page in range(1, num_pages + 1):\n",
    "    print(f\"Scraping page {page}...\")\n",
    "    # Construct the URL for the current page\n",
    "    url = f\"{url}?page={page}\"\n",
    "    \n",
    "    # Fetch the page content\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch page {page}\")\n",
    "        continue\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Extract product titles (adjust the class name as per your HTML structure)\n",
    "    products = soup.find_all('div', class_='KzDlHZ')  # Replace with the actual class name\n",
    "    for product in products:\n",
    "        title = product.get_text(strip=True)\n",
    "        all_product_names.append(title)\n",
    "\n",
    "# Print all product names\n",
    "print(\"\\nAll Product Names:\")\n",
    "for name in all_product_names:\n",
    "    print(name)\n",
    "\n",
    "# Optionally, save to a file\n",
    "with open(\"product_names.txt\", \"w\") as file:\n",
    "    file.write(\"\\n\".join(all_product_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2e2b14-4304-406e-89e9-9e155b53b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_products = len(all_product_names)\n",
    "print(\"Total number of product:\", total_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d585102a-a10a-49ea-b0aa-154f0ea703a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_prices = []\n",
    "\n",
    "num_pages = 39\n",
    "\n",
    "for page in range(1, num_pages + 1):\n",
    "    print(f\"Scraping page {page}...\")\n",
    "    \n",
    "    page_url = f\"{url}&page={page}\"  # use base_url, not reassigning url\n",
    "    \n",
    "    response = requests.get(page_url, headers={\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36'\n",
    "    })\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch page {page}\")\n",
    "        continue\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Corrected class for prices\n",
    "    prices = soup.find_all('div', class_='Nx9bqj _4b5DiR')\n",
    "    \n",
    "    for price in prices:\n",
    "        product_prices.append(price.get_text(strip=True))\n",
    "\n",
    "# Output and save\n",
    "print(\"\\nAll Price Names:\")\n",
    "for name in product_prices:\n",
    "    print(name)\n",
    "\n",
    "with open(\"price_names.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(product_prices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9d251d-6348-4cac-a6d5-6ea47cdeb85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_prices = len(product_prices)\n",
    "print(\"Total number of prices:\", total_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e876782-c074-4ddf-98c8-8c5d93382056",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_ratings = []\n",
    "\n",
    "# Number of pages to scrape (you can adjust this or determine dynamically)\n",
    "num_pages = 39\n",
    "\n",
    "# Loop through each page\n",
    "for page in range(1, num_pages + 1):\n",
    "    print(f\"Scraping page {page}...\")\n",
    "    # Construct the URL for the current page\n",
    "    url = f\"{url}?page={page}\"\n",
    "    \n",
    "    # Fetch the page content\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch page {page}\")\n",
    "        continue\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Extract price titles (adjust the class name as per your HTML structure)\n",
    "    ratings = soup.find_all('div', class_='XQDdHH')  # Replace with the actual class name\n",
    "    for rating in ratings:\n",
    "        title = rating.get_text(strip=True)\n",
    "        product_ratings.append(title)\n",
    "\n",
    "# Print all price names\n",
    "print(\"\\nAll ratings:\")\n",
    "for name in product_ratings:\n",
    "    print(name)\n",
    "\n",
    "# Optionally, save to a file\n",
    "with open(\"ratings.txt\", \"w\") as file:\n",
    "    file.write(\"\\n\".join(product_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a4bf30-05ed-4df4-9153-ba718ba21d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ratings = len(product_ratings)\n",
    "print(\"Total number of ratings:\", total_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfd5eee-6ed3-40bf-865e-83171407db6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_discount = []\n",
    "\n",
    "# Number of pages to scrape (you can adjust this or determine dynamically)\n",
    "num_pages = 39\n",
    "\n",
    "# Loop through each page\n",
    "for page in range(1, num_pages + 1):\n",
    "    print(f\"Scraping page {page}...\")\n",
    "    # Construct the URL for the current page\n",
    "    url = f\"{url}?page={page}\"\n",
    "    \n",
    "    # Fetch the page content\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch page {page}\")\n",
    "        continue\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Extract price titles (adjust the class name as per your HTML structure)\n",
    "    dis = soup.find_all('div', class_='UkUFwK')  # Replace with the actual class name\n",
    "    for discount in dis:\n",
    "        title = discount.get_text(strip=True)\n",
    "        product_discount.append(title)\n",
    "\n",
    "# Print all price names\n",
    "print(\"\\nAll ratings:\")\n",
    "for name in product_discount:\n",
    "    print(name)\n",
    "\n",
    "# Optionally, save to a file\n",
    "with open(\"discount.txt\", \"w\") as file:\n",
    "    file.write(\"\\n\".join(product_discount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e66d574-60f7-4185-9f12-dc62768f8b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_discount = len(product_discount)\n",
    "print(\"Total number of discount:\", total_discount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128626bf-54c6-49b5-b695-e97e9d2eda0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Product names:\", len(all_product_names))\n",
    "print(\"Prices:\", len(product_prices))\n",
    "print(\"Discounts:\", len(product_discount))\n",
    "print(\"Ratings:\", len(product_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94ecb26-7778-4c8d-beb1-8393f21f3c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_length = min(len(all_product_names), len(product_prices), len(product_discount), len(product_ratings))\n",
    "\n",
    "data = {\n",
    "    \"Product Name\": all_product_names[:min_length],\n",
    "    \"Price\": product_prices[:min_length],\n",
    "    \"Discount\": product_discount[:min_length],\n",
    "    \"Rating\": product_ratings[:min_length]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d5d057-a552-40a1-ad32-e534b4d5eb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import zip_longest\n",
    "\n",
    "data = list(zip_longest(all_product_names, product_prices, product_discount, product_ratings, fillvalue=\"NA\"))\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"Product Name\", \"Price\", \"Discount\", \"Rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d67e0ef-2c6c-438d-a33f-944659dd9e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b48771-151f-4f25-adad-4d22cb8054ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
